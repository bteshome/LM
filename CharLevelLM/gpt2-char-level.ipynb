{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dba7f6-8213-4ed9-a48a-1ef4bce53a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd00b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2 #8\n",
    "block_size = 48 #1024\n",
    "dimension = 36 #768\n",
    "num_heads = 12\n",
    "n_layers = 4 #12\n",
    "max_epochs = 2000 #2500\n",
    "lr = 1e-3\n",
    "dropout = 0.1\n",
    "topk = 50\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'\n",
    "torch.manual_seed(99)\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31f20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n",
      "First 50 characters: First Citizen:\n",
      "Before we proceed any further, hear\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(\"Length of dataset in characters:\", len(text))\n",
    "print(f\"First {50} characters: {text[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1913cb62-6900-4d4a-9c0a-b825a41c068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique characters sorted: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Number of unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"All unique characters sorted: {''.join(chars)}\")\n",
    "print(f\"Number of unique characters: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b608af3-bce7-434a-a7a3-cca9b4c377f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda ch : [stoi[c] for c in ch]\n",
    "decode = lambda ix : ''.join([itos[i] for i in ix])\n",
    "print(encode('hi there'))\n",
    "print(decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab89221-7e84-429a-bb85-6e8eb0d072ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc748b1-a778-4f22-9039-78dfd192c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854])\n",
      "torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1423ac2a-9d18-4a68-b6b3-0cd4e0b7ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when context is tensor([18]), next token is: 47\n",
      "when context is tensor([18, 47]), next token is: 56\n",
      "when context is tensor([18, 47, 56]), next token is: 57\n",
      "when context is tensor([18, 47, 56, 57]), next token is: 58\n",
      "when context is tensor([18, 47, 56, 57, 58]), next token is: 1\n",
      "when context is tensor([18, 47, 56, 57, 58,  1]), next token is: 15\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15]), next token is: 47\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47]), next token is: 58\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]), next token is: 47\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]), next token is: 64\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]), next token is: 52\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]), next token is: 10\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]), next token is: 0\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]), next token is: 14\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43]), next token is: 44\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44]), next token is: 53\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53]), next token is: 56\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43]), next token is: 1\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1]), next token is: 61\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43]), next token is: 1\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1]), next token is: 54\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54]), next token is: 56\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56]), next token is: 53\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53]), next token is: 41\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43]), next token is: 42\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42]), next token is: 1\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1]), next token is: 39\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39]), next token is: 52\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52]), next token is: 63\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63]), next token is: 1\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1]), next token is: 44\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44]), next token is: 59\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59]), next token is: 56\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56]), next token is: 58\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58]), next token is: 46\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43]), next token is: 56\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56]), next token is: 6\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6]), next token is: 1\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1]), next token is: 46\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46]), next token is: 43\n",
      "when context is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43]), next token is: 39\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when context is {context}, next token is: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "995fb0e7-c40f-4076-8600-c42ae4b29d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch - x: torch.Size([2, 48])\n",
      "tensor([[63,  8,  0,  0, 18, 47, 56, 57, 58,  1, 31, 43, 56, 60, 39, 52, 58, 10,\n",
      "          0, 37, 53, 59,  1, 39, 56, 43,  1, 50, 53, 53, 49, 43, 42,  1, 44, 53,\n",
      "         56,  1, 39, 52, 42,  1, 41, 39, 50, 50, 43, 42],\n",
      "        [59, 42, 45, 51, 43, 52, 58,  1, 60, 39, 52, 47, 57, 46,  5, 42,  1, 44,\n",
      "         56, 53, 51,  1, 46, 47, 57,  1, 50, 47, 54, 57,  6,  0, 26, 53, 58,  1,\n",
      "         40, 53, 42, 63,  5, 57,  1, 42, 43, 39, 58, 46]])\n",
      "batch - y: torch.Size([2, 48])\n",
      "tensor([[ 8,  0,  0, 18, 47, 56, 57, 58,  1, 31, 43, 56, 60, 39, 52, 58, 10,  0,\n",
      "         37, 53, 59,  1, 39, 56, 43,  1, 50, 53, 53, 49, 43, 42,  1, 44, 53, 56,\n",
      "          1, 39, 52, 42,  1, 41, 39, 50, 50, 43, 42,  1],\n",
      "        [42, 45, 51, 43, 52, 58,  1, 60, 39, 52, 47, 57, 46,  5, 42,  1, 44, 56,\n",
      "         53, 51,  1, 46, 47, 57,  1, 50, 47, 54, 57,  6,  0, 26, 53, 58,  1, 40,\n",
      "         53, 42, 63,  5, 57,  1, 42, 43, 39, 58, 46,  6]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data_source='train'):\n",
    "    source = train_data if data_source == 'train' else val_data \n",
    "    ix = torch.randint(low=0, high=len(source) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([source[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([source[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('batch - x:', xb.shape)\n",
    "print(xb)\n",
    "print('batch - y:', yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ff5c61-911b-4bc8-9fef-badeb649242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, dimension_head):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.query_layer = nn.Linear(dimension, dimension_head, bias=False)\n",
    "        self.key_layer = nn.Linear(dimension, dimension_head, bias=False)\n",
    "        self.value_layer = nn.Linear(dimension, dimension_head, bias=False)\n",
    "        self.scale = dimension_head ** 0.5\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, T, _ = x.shape\n",
    "        assert T <= block_size, f\"Input length exceeds block size {block_size}.\"\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "\n",
    "        K_T = K.transpose(1, 2)\n",
    "        attention_scores = Q @ K_T\n",
    "        attention_scores = attention_scores / self.scale\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        output = attention_weights @ V   \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert dimension % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        dimension_head = dimension // num_heads  \n",
    "        self.heads = nn.ModuleList([AttentionHead(dimension_head) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(dimension, dimension)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        head_outputs = [head(x) for head in self.heads]                \n",
    "        output = torch.cat(head_outputs, dim=-1)\n",
    "        output = self.proj(output)                                       \n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dimension, 4*dimension,),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4*dimension, dimension)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dimension)\n",
    "        self.sa = MultiHeadAttention()\n",
    "        self.ln2 = nn.LayerNorm(dimension)\n",
    "        self.ff = FeedForward()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.sa(self.ln1(x)))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, dimension)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, dimension)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layers)])\n",
    "        self.lm_head = nn.Linear(dimension, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        _, T_idx = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos = torch.arange(T_idx, device=device)\n",
    "        pos_emb = self.position_embedding_table(pos)\n",
    "        pos_emb = pos_emb.unsqueeze(0)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits_flattened = logits.view(-1, logits.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_flattened, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]                              \n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            topk_probs, topk_indices = torch.topk(probs, k=topk, dim=-1)\n",
    "            idx_next = torch.multinomial(topk_probs, num_samples=1)\n",
    "            xcol = torch.gather(topk_indices, dim=-1, index=idx_next)\n",
    "            idx = torch.cat((idx, xcol), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d04a4cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48, 65])\n",
      "tensor(4.5552, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa61e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ";AqDAJdM'XH!p&&tuLARVdmrijt\n",
      "Db;k&BpxoO,op'3dcqeRAo\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = model.generate(idx, 50)[0].tolist()\n",
    "print(decode(generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a8647cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for epoch in range(max_epochs):\n",
    "        xb, yb = get_batch('train')\n",
    "        _, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 500 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d79d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.518505573272705\n",
      "Epoch 500, Loss: 2.4197661876678467\n",
      "Epoch 1000, Loss: 2.6544950008392334\n",
      "Epoch 1500, Loss: 2.662320137023926\n",
      "Epoch 2000, Loss: 2.6340301036834717\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2b9fcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path = 'gpt-char-level.pth'\n",
    "torch.save(model.state_dict(), model_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b813052e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('token_embedding_table.weight',\n",
       "              tensor([[ 1.1282,  0.0148,  1.0783,  ...,  0.9926, -0.4567, -1.9195],\n",
       "                      [ 0.0685,  1.2233, -1.0039,  ...,  1.5837,  0.0771,  1.3063],\n",
       "                      [-0.6846, -0.6239, -0.7813,  ..., -1.8358, -0.9043, -1.3798],\n",
       "                      ...,\n",
       "                      [-1.2493, -0.0464, -0.1076,  ...,  0.9245, -0.3058,  0.5545],\n",
       "                      [-0.9794, -1.2060, -1.0753,  ..., -1.2805, -1.4240,  1.2248],\n",
       "                      [ 1.1210,  1.2918,  0.8996,  ...,  0.7433, -0.3463, -0.7676]])),\n",
       "             ('position_embedding_table.weight',\n",
       "              tensor([[-1.7494e+00,  1.6451e+00, -1.1817e+00, -6.3707e-01, -1.4959e+00,\n",
       "                        1.5132e+00,  7.4200e-01, -1.2802e-01,  2.6503e-02,  8.3261e-01,\n",
       "                       -4.8534e-01, -1.2051e+00,  1.2452e+00, -1.2536e+00,  4.0152e-02,\n",
       "                        8.3950e-01,  5.9007e-01,  6.0966e-01, -7.0797e-01, -3.8498e-01,\n",
       "                        1.2649e+00,  9.3382e-02, -4.1492e-01, -6.1775e-01,  6.6473e-01,\n",
       "                        1.0145e+00,  4.8154e-01, -1.5916e+00, -6.0310e-01, -2.5179e-02,\n",
       "                       -3.2978e-05,  4.7788e-01],\n",
       "                      [ 1.3897e+00, -1.7526e+00, -1.1510e+00,  8.7681e-01, -5.3032e-01,\n",
       "                        2.5206e-01,  3.1909e-01,  2.9528e-01, -9.4533e-01, -6.1530e-01,\n",
       "                        8.2757e-02, -3.8030e-01,  6.8880e-01,  1.4559e+00,  2.0394e+00,\n",
       "                       -2.0582e+00, -5.8986e-01,  1.5777e+00,  3.7166e-01,  1.3653e+00,\n",
       "                        7.3664e-01,  5.6700e-01, -2.1845e-01,  6.6817e-01, -8.2450e-01,\n",
       "                       -5.1452e-01, -5.0480e-01,  8.7905e-01,  5.2176e-01, -5.3460e-01,\n",
       "                        4.0723e-01,  1.2703e+00],\n",
       "                      [-8.8669e-01, -1.1992e+00,  1.3617e+00,  1.0990e+00,  6.6978e-02,\n",
       "                        1.0239e+00, -1.5817e+00,  1.0242e+00,  8.4622e-01,  1.1822e+00,\n",
       "                        2.0471e+00,  1.3615e-01, -3.8091e-01,  9.5016e-01, -1.6667e+00,\n",
       "                       -1.2027e+00,  1.7752e+00, -1.2900e+00,  6.3220e-01, -2.2582e-01,\n",
       "                       -1.1439e+00, -7.3077e-01,  1.5282e+00, -5.7895e-01, -1.8367e-01,\n",
       "                       -6.8928e-01,  5.3605e-01,  7.7542e-03,  1.2535e+00,  1.9680e+00,\n",
       "                       -2.4016e+00,  4.7211e-01],\n",
       "                      [-4.4423e-01, -1.0869e-01,  1.6039e-01,  2.6031e-02, -1.3746e+00,\n",
       "                       -1.0986e+00, -2.2612e-01,  2.1509e+00,  8.7764e-02, -3.5427e-01,\n",
       "                        1.5475e+00,  5.1254e-01, -4.2136e-01,  6.5713e-01,  1.0780e+00,\n",
       "                        1.7359e+00, -4.4837e-01, -9.5877e-01, -1.4608e-01, -5.2394e-01,\n",
       "                        3.1368e-01,  1.3586e+00, -3.1437e-01, -7.9385e-01, -1.2037e-01,\n",
       "                        1.2491e+00, -8.9522e-01,  7.1800e-01,  1.3828e-01, -6.7753e-01,\n",
       "                       -1.1841e+00, -3.4817e-01],\n",
       "                      [ 2.9419e-01,  7.6265e-01,  2.6536e+00,  4.7298e-01,  1.4674e-02,\n",
       "                       -1.1246e-01,  4.7136e-01, -9.2004e-01, -4.6132e-01,  2.0802e+00,\n",
       "                        8.3091e-01, -8.4160e-01, -1.6443e-01, -3.0152e+00,  6.6938e-01,\n",
       "                        1.0419e-01, -5.6244e-01, -1.9780e-01,  8.1014e-01, -1.0031e+00,\n",
       "                        6.1045e-01, -5.1126e-01,  6.3174e-01, -5.7605e-01, -1.0993e+00,\n",
       "                        1.3919e+00,  9.9440e-01,  5.4530e-01,  1.6932e-01, -8.0326e-01,\n",
       "                       -3.5394e-01,  1.0305e+00],\n",
       "                      [ 6.6229e-01, -1.6373e+00,  5.7993e-01, -6.8153e-02,  5.8550e-01,\n",
       "                        9.1146e-01,  5.0209e-01, -9.9619e-01, -2.8114e+00,  1.0641e-01,\n",
       "                       -1.1193e+00,  3.5147e-02,  4.1467e-01,  1.7470e+00, -1.4547e-01,\n",
       "                        1.1585e-01,  1.3345e+00, -7.4055e-02, -1.3989e+00, -8.8319e-01,\n",
       "                        5.8570e-01,  1.0718e+00,  3.6993e-01, -3.1272e-02,  6.6583e-01,\n",
       "                        1.7044e-01,  6.5845e-01,  1.0239e+00,  5.7372e-01,  4.0579e-01,\n",
       "                       -2.4139e-01,  1.8125e-01],\n",
       "                      [-4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00, -1.6080e+00,\n",
       "                       -2.9419e-01, -2.6721e+00,  2.2989e-01, -5.7089e-01,  2.3408e+00,\n",
       "                       -2.5665e+00,  9.9348e-01,  1.6282e+00, -1.2730e-01, -1.2886e+00,\n",
       "                        2.1739e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,  2.7992e-01,\n",
       "                        5.3649e-01, -1.3393e+00,  1.3857e-01, -9.1848e-02, -3.0254e-01,\n",
       "                        5.5197e-01, -5.9396e-01,  7.7175e-01,  5.4131e-01,  5.9909e-01,\n",
       "                        5.4487e-01, -1.3661e-01],\n",
       "                      [-4.7539e-01,  8.2144e-02,  2.2645e+00, -1.2837e+00, -1.4874e+00,\n",
       "                        1.4235e+00,  8.7337e-01,  5.5404e-02,  1.4642e-01,  3.0068e-01,\n",
       "                       -6.9550e-01, -1.0944e+00,  6.1771e-02,  4.5541e-01, -1.3190e+00,\n",
       "                        1.0053e-01,  1.3060e+00, -1.2617e+00, -6.6971e-02,  2.9986e-01,\n",
       "                       -2.1312e+00,  5.5584e-01, -1.5796e+00,  9.4423e-01, -3.3658e-01,\n",
       "                        3.0248e-01, -4.2422e-01,  2.3600e-01, -1.0956e-02,  2.7496e+00,\n",
       "                        1.7543e-01,  1.2992e+00]])),\n",
       "             ('sa_head.tril',\n",
       "              tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "                      [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "                      [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "                      [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "                      [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "                      [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "                      [1., 1., 1., 1., 1., 1., 1., 1.]])),\n",
       "             ('sa_head.key.weight',\n",
       "              tensor([[-1.2744e-01, -9.0591e-02, -3.2721e-02,  1.5281e-01, -1.1530e-01,\n",
       "                       -2.1771e-02,  1.2018e-01, -1.0511e-01,  3.1425e-02, -5.6275e-02,\n",
       "                       -1.1823e-01,  9.6793e-02, -1.3880e-01, -4.3594e-02,  8.1095e-02,\n",
       "                       -1.3598e-01, -2.9257e-02,  1.4066e-01,  1.2006e-02, -2.2540e-02,\n",
       "                        6.3517e-02, -1.1260e-01, -1.6291e-02, -6.3192e-02, -4.9987e-02,\n",
       "                        1.7431e-01, -6.0255e-02,  1.0112e-01,  1.5938e-01, -9.7368e-02,\n",
       "                       -5.2224e-02, -7.7102e-04],\n",
       "                      [-7.3207e-02, -2.2474e-02, -1.5954e-01,  9.8654e-02, -3.6335e-02,\n",
       "                       -1.0507e-01,  1.0686e-01, -1.3612e-01,  4.2255e-03, -1.2568e-01,\n",
       "                        2.8030e-03, -8.1609e-02,  1.3071e-01, -9.3097e-02,  1.0167e-01,\n",
       "                       -6.1183e-02, -1.1983e-01, -1.6064e-01,  8.6483e-02,  1.7503e-02,\n",
       "                       -8.2136e-03,  1.2145e-01,  2.9709e-02,  1.6798e-01, -8.2597e-02,\n",
       "                        4.6789e-02,  1.4055e-01,  1.1965e-01, -1.4672e-01, -2.2842e-02,\n",
       "                        7.6391e-02,  6.1046e-02],\n",
       "                      [-1.7482e-01, -1.9072e-02, -1.0261e-01,  6.7306e-02,  5.2695e-02,\n",
       "                        3.3354e-02, -1.6348e-01, -1.3348e-01, -6.1817e-02, -2.4414e-02,\n",
       "                       -8.5281e-02, -1.6668e-01,  1.2236e-01, -1.0346e-01,  5.2147e-02,\n",
       "                        2.2436e-02,  1.4633e-01, -2.3826e-02, -9.1505e-02, -1.3977e-01,\n",
       "                       -5.1269e-02, -1.4557e-04, -9.1440e-02,  8.5166e-02, -1.5078e-01,\n",
       "                       -1.7439e-01, -5.6068e-02, -7.4769e-02,  1.3896e-01, -1.4227e-01,\n",
       "                        1.0046e-01,  9.0056e-02],\n",
       "                      [-9.3704e-02, -2.7463e-05,  2.7572e-02,  1.3548e-02, -5.4878e-02,\n",
       "                       -2.6469e-03,  1.4287e-01, -1.3188e-02, -7.1960e-02,  9.1345e-02,\n",
       "                        1.5275e-01, -5.9872e-02, -1.1151e-01,  5.8637e-03, -1.2234e-01,\n",
       "                       -5.0603e-02, -8.9582e-02,  4.0823e-02, -8.6493e-03, -8.5673e-02,\n",
       "                        1.2097e-01,  3.2457e-02, -1.6457e-01, -1.6276e-01,  1.3655e-01,\n",
       "                        1.0563e-01, -2.3120e-02,  2.3684e-02,  6.9023e-02,  1.6231e-01,\n",
       "                        1.6161e-01, -1.4180e-02],\n",
       "                      [-1.4356e-01, -2.1864e-02,  1.0744e-01, -7.6470e-02, -1.0747e-01,\n",
       "                        6.4065e-02,  1.3644e-01,  1.2309e-01,  1.0846e-01,  1.7344e-01,\n",
       "                        2.1286e-02, -1.5374e-01, -2.3477e-02,  4.3419e-02,  1.3314e-01,\n",
       "                        5.5641e-02,  5.5979e-02,  8.5726e-02,  3.2915e-02, -4.3203e-02,\n",
       "                        8.2590e-02, -1.4975e-01,  1.0440e-01,  1.7637e-01, -1.3977e-01,\n",
       "                       -2.2259e-02,  1.0861e-02, -2.8268e-03, -6.7115e-02, -5.0956e-02,\n",
       "                       -6.5434e-02,  1.3788e-01],\n",
       "                      [ 8.3421e-02, -6.3037e-02, -1.1836e-01, -1.3337e-01, -6.8174e-02,\n",
       "                        1.6344e-01, -1.0402e-01, -3.9746e-02,  1.5380e-01, -1.0935e-01,\n",
       "                       -1.3097e-01, -1.0422e-02, -1.8089e-02, -3.5575e-02,  1.6380e-01,\n",
       "                       -9.8920e-02, -1.3306e-01, -2.9678e-02, -9.8845e-02,  9.5659e-02,\n",
       "                        1.3952e-01,  7.4458e-02,  2.0658e-02,  1.2132e-01, -3.6998e-02,\n",
       "                        1.7040e-02, -3.5530e-04,  2.8157e-02, -5.5661e-03,  1.7634e-01,\n",
       "                        5.9439e-02,  2.3643e-02],\n",
       "                      [ 2.1312e-02, -6.7824e-02,  1.1029e-01,  8.9882e-02, -6.5131e-02,\n",
       "                       -6.7521e-02,  1.3713e-01,  6.3793e-02, -1.4960e-01,  1.4877e-01,\n",
       "                       -5.0328e-02,  1.3102e-01,  1.3719e-01, -6.1329e-02,  1.2499e-01,\n",
       "                        7.2348e-03, -1.1680e-01, -2.1684e-02,  1.6699e-01,  4.0605e-02,\n",
       "                        3.5740e-02, -1.2499e-01,  1.6242e-01, -7.5537e-02, -1.1510e-01,\n",
       "                        5.2012e-02, -1.0990e-02, -6.1587e-03, -1.3278e-01, -1.2726e-02,\n",
       "                       -6.5043e-03,  1.3108e-01],\n",
       "                      [-1.2734e-01,  3.9135e-02, -4.3122e-02,  2.8592e-02, -8.4026e-02,\n",
       "                        1.5125e-01,  3.9573e-03, -1.5036e-01,  8.5413e-02, -5.2741e-02,\n",
       "                        1.5607e-01, -2.9856e-02, -3.7681e-02, -1.3298e-01, -2.5065e-02,\n",
       "                       -1.6690e-01,  1.2162e-01,  1.4547e-02, -1.4342e-01,  1.3916e-01,\n",
       "                        8.3353e-02,  1.1979e-01,  1.2251e-01,  1.6394e-01, -1.4082e-01,\n",
       "                       -5.5506e-02, -7.3543e-03,  1.2489e-01,  5.9592e-02, -2.6162e-03,\n",
       "                        1.5028e-01, -1.5019e-01],\n",
       "                      [-1.1077e-02, -1.2279e-01,  4.6597e-03, -4.8722e-02, -1.0374e-01,\n",
       "                       -1.5623e-01,  7.6362e-02, -7.8237e-02, -1.6420e-01,  8.8269e-03,\n",
       "                        1.3668e-01,  7.8241e-02,  5.4937e-02, -1.4771e-01, -8.4544e-02,\n",
       "                       -1.4250e-01,  1.5942e-01, -1.1538e-01, -7.2147e-02,  4.7057e-02,\n",
       "                       -1.5723e-02, -1.3861e-01,  1.3110e-01, -5.8933e-02, -1.3362e-01,\n",
       "                       -1.0700e-01,  1.0167e-02,  4.9537e-04, -4.5115e-02,  1.1943e-01,\n",
       "                        1.3910e-01,  1.3870e-01],\n",
       "                      [ 1.7392e-03,  8.4881e-03, -7.0957e-02, -1.5127e-01, -1.3595e-01,\n",
       "                        5.8609e-02, -1.3244e-02, -1.5638e-01, -1.2652e-01,  1.5329e-01,\n",
       "                       -1.4260e-01,  9.2699e-02,  5.5842e-02, -7.0915e-03, -1.3180e-01,\n",
       "                       -1.4011e-01,  1.5666e-01,  8.0631e-02, -1.2700e-01,  1.4796e-01,\n",
       "                       -7.6413e-02, -1.6245e-01,  5.5341e-02, -1.1180e-01, -1.4595e-01,\n",
       "                       -5.9422e-02,  3.7485e-02,  9.5968e-02, -1.3351e-01,  2.6567e-03,\n",
       "                       -6.2531e-02,  1.4939e-01],\n",
       "                      [-1.0552e-01,  8.5097e-02, -3.5802e-02, -9.4162e-02, -1.6033e-01,\n",
       "                        9.0268e-02,  7.0796e-02, -7.8222e-03,  8.8142e-02,  2.9308e-02,\n",
       "                       -1.3417e-01,  1.1699e-01, -1.5547e-01, -1.1994e-01, -1.7672e-01,\n",
       "                        7.8591e-02,  1.1656e-01,  5.4467e-02, -9.5190e-02,  9.2754e-02,\n",
       "                        1.5141e-01,  1.4220e-01,  6.5540e-02,  5.7491e-03, -1.6839e-01,\n",
       "                       -2.7127e-02, -7.5906e-02,  9.8774e-02, -6.5393e-03,  1.5705e-01,\n",
       "                        4.8395e-02, -1.1416e-01],\n",
       "                      [ 1.2176e-01,  1.4901e-01, -1.5136e-01,  1.1442e-01,  2.6820e-02,\n",
       "                        1.5503e-01, -1.0916e-01,  9.1822e-02, -1.7254e-01,  1.4207e-01,\n",
       "                        3.3879e-02,  9.2135e-02, -1.7341e-01, -1.5899e-01,  1.6237e-01,\n",
       "                       -3.9110e-02,  6.2611e-02, -4.5505e-03, -3.9370e-02,  2.4946e-02,\n",
       "                        1.1485e-01,  1.2543e-01, -2.6573e-02, -1.1586e-01, -7.5895e-02,\n",
       "                       -1.5411e-01, -5.8887e-02, -1.1766e-03, -8.4793e-02, -8.2502e-02,\n",
       "                       -4.4435e-03, -5.5434e-02],\n",
       "                      [-1.0337e-01, -3.1764e-02,  6.3003e-03, -4.2740e-02,  1.2742e-01,\n",
       "                       -7.2587e-02, -1.0991e-01, -1.2501e-01,  1.0049e-01,  2.2924e-02,\n",
       "                       -6.0060e-02, -1.3100e-01,  1.5237e-01,  1.0492e-01, -1.0672e-01,\n",
       "                       -4.6122e-02, -8.1510e-02, -1.0007e-01,  3.8672e-02, -8.2588e-02,\n",
       "                        1.4151e-01,  6.0314e-02,  1.2568e-01, -4.5407e-02,  1.3438e-02,\n",
       "                        3.9865e-02,  1.7466e-01,  3.3583e-02, -1.4581e-01,  6.5601e-03,\n",
       "                        4.5544e-03,  8.3425e-02],\n",
       "                      [-9.3671e-02,  9.5349e-02, -7.2890e-02,  4.9051e-02,  1.0999e-01,\n",
       "                       -1.5315e-02,  1.2052e-01,  1.9252e-03,  3.6491e-02,  3.4950e-03,\n",
       "                       -1.4084e-01, -2.8148e-02,  1.2539e-01,  8.6967e-02,  5.4905e-02,\n",
       "                        5.4550e-02,  1.0856e-01, -1.4128e-02, -1.6265e-02,  4.8114e-02,\n",
       "                       -1.4708e-01,  1.3609e-01,  1.4714e-01,  1.5474e-01, -1.3667e-01,\n",
       "                       -7.2933e-02,  1.2425e-01,  9.3911e-02, -1.4735e-01,  1.0617e-01,\n",
       "                        2.9284e-03, -1.1240e-01],\n",
       "                      [-1.6581e-01, -1.3826e-02, -7.0664e-02, -7.4552e-02, -4.7481e-02,\n",
       "                        7.8537e-02, -3.4278e-02, -3.1041e-02,  1.9947e-02,  1.3531e-01,\n",
       "                        4.5143e-02, -2.4004e-02,  1.0170e-01, -1.2700e-01,  8.5834e-02,\n",
       "                        1.2374e-01,  7.3101e-02,  3.8742e-02, -1.3298e-01,  1.3267e-01,\n",
       "                       -1.0574e-01,  1.3348e-01, -7.1331e-02, -8.5805e-02, -2.7066e-02,\n",
       "                        1.5651e-01,  3.8676e-02, -1.6794e-01,  2.7627e-02, -9.9581e-02,\n",
       "                       -5.8991e-02, -6.7403e-02],\n",
       "                      [ 1.6197e-02, -9.1134e-02, -3.7999e-02, -9.3017e-02, -7.0086e-02,\n",
       "                       -2.3121e-02, -6.7197e-02,  1.4868e-01, -9.5179e-03,  1.2089e-01,\n",
       "                       -1.7563e-01,  2.7205e-02,  1.5252e-01,  1.7419e-01,  5.4032e-02,\n",
       "                       -1.4230e-01,  1.6949e-01, -3.9026e-02,  8.3696e-02, -7.7452e-02,\n",
       "                       -1.2784e-01, -1.2128e-01,  1.5384e-01,  5.2271e-02,  9.8340e-02,\n",
       "                       -1.2927e-01, -2.5392e-03, -1.4362e-02, -3.4582e-02,  1.4733e-01,\n",
       "                        3.0865e-02, -4.5137e-02]])),\n",
       "             ('sa_head.query.weight',\n",
       "              tensor([[ 3.3325e-02,  1.3553e-01, -5.0467e-02,  1.7204e-01,  5.6159e-02,\n",
       "                        1.5956e-01,  4.5189e-02,  9.4508e-02, -1.2600e-01, -1.3118e-01,\n",
       "                        3.3453e-02, -1.7226e-01,  1.6451e-01,  6.4146e-02,  8.8674e-02,\n",
       "                       -1.2932e-01,  7.7820e-02, -2.8590e-02,  1.3794e-01,  3.5847e-02,\n",
       "                       -1.1839e-01, -1.7212e-01, -6.7307e-02, -3.6860e-02,  9.9998e-02,\n",
       "                        1.1097e-01, -1.1587e-01, -7.2029e-03,  8.6381e-02,  6.6926e-02,\n",
       "                        1.5929e-01,  1.5981e-01],\n",
       "                      [-9.7649e-02, -5.3871e-02,  1.5451e-01, -6.0663e-02,  5.8953e-02,\n",
       "                        1.5725e-01, -4.7909e-02, -3.4434e-02,  1.1934e-02, -1.5325e-01,\n",
       "                        1.7121e-01,  1.2748e-01,  1.2018e-01, -2.1043e-02,  8.3018e-02,\n",
       "                        1.0941e-01,  8.6074e-02,  1.2525e-01,  1.0729e-01, -1.6595e-01,\n",
       "                       -1.4365e-01,  1.1479e-01,  1.2650e-01, -1.3575e-01,  3.5505e-03,\n",
       "                       -3.0967e-02, -6.5193e-02,  4.4470e-02,  5.9381e-02, -3.0173e-02,\n",
       "                        1.1501e-01,  1.5654e-01],\n",
       "                      [ 2.9350e-02, -1.0171e-01, -1.5615e-01, -8.6595e-02,  8.2045e-03,\n",
       "                        7.0731e-02, -9.4401e-02, -1.3788e-01, -9.7983e-02, -7.2489e-02,\n",
       "                       -1.2726e-01,  1.7377e-01, -6.1797e-02, -7.9188e-02, -1.0417e-01,\n",
       "                        3.5767e-02,  1.7465e-01,  9.0207e-02,  1.5754e-01,  3.5117e-02,\n",
       "                        6.1316e-02,  9.3838e-03,  4.7495e-02, -2.0945e-02, -8.6444e-02,\n",
       "                        2.4117e-02, -3.3034e-03, -1.1545e-01,  1.2361e-01,  8.8045e-02,\n",
       "                        3.3702e-02,  1.1668e-01],\n",
       "                      [-2.1931e-02, -1.4017e-01,  2.2782e-02,  1.4678e-01, -1.1368e-01,\n",
       "                        1.2961e-01, -1.2188e-01, -1.0561e-01, -1.6153e-01,  1.5229e-01,\n",
       "                       -7.1044e-02, -2.4135e-04, -7.6884e-02,  1.4195e-01,  4.8875e-02,\n",
       "                        2.5951e-02, -6.5747e-02, -1.4099e-02, -9.4966e-02,  1.9044e-02,\n",
       "                        1.0393e-01, -1.5518e-01,  1.6191e-01,  1.3906e-01, -8.8302e-02,\n",
       "                       -1.3212e-01, -6.4917e-02, -1.6834e-01,  1.0054e-01, -8.5452e-02,\n",
       "                       -6.0699e-02, -5.4569e-02],\n",
       "                      [ 1.3656e-01, -8.9074e-02,  3.8166e-02,  1.6421e-01,  1.6411e-01,\n",
       "                       -1.7509e-01, -7.1104e-02,  3.8814e-02,  1.4584e-01, -1.4684e-02,\n",
       "                       -5.1858e-02, -4.7328e-03,  1.7294e-01,  5.6623e-02, -1.5693e-01,\n",
       "                       -1.3398e-01,  1.0942e-01,  7.0628e-03,  2.2548e-02,  7.5414e-03,\n",
       "                        4.2419e-03, -1.7594e-01,  5.4045e-02,  7.1441e-02,  9.6454e-02,\n",
       "                        3.6563e-02,  9.3471e-02, -4.6056e-02,  6.6643e-03,  1.0332e-01,\n",
       "                        9.3687e-02,  1.7535e-01],\n",
       "                      [-1.2184e-01,  2.7231e-02, -5.6242e-03,  1.1011e-01,  1.4769e-01,\n",
       "                       -5.5340e-02, -1.5592e-02,  6.4836e-02, -1.0642e-01, -9.0670e-02,\n",
       "                       -6.0771e-03,  7.9640e-02, -1.3312e-02, -1.2988e-01, -1.3477e-01,\n",
       "                       -7.9902e-02,  6.1300e-02,  1.1593e-01, -3.6705e-03, -8.3052e-02,\n",
       "                        1.0216e-01,  3.5247e-02,  1.2140e-01,  4.4787e-02, -1.4407e-01,\n",
       "                       -1.1007e-01, -2.8229e-02, -1.2591e-01,  1.4328e-01,  9.7547e-04,\n",
       "                        1.2109e-01,  1.0573e-01],\n",
       "                      [-2.7426e-02, -6.3149e-02, -4.5443e-02,  1.7446e-01,  1.3225e-01,\n",
       "                       -7.1434e-02,  3.5053e-02,  1.0283e-01,  3.0920e-02,  1.1238e-01,\n",
       "                       -1.2410e-01, -1.4595e-01, -1.5164e-01, -9.2925e-02, -3.9750e-02,\n",
       "                        3.8119e-02,  6.3676e-03,  4.7958e-03,  1.2782e-01,  1.1431e-01,\n",
       "                       -8.2161e-02, -1.7530e-03,  4.0007e-02, -5.6558e-02, -6.9031e-02,\n",
       "                       -1.0924e-01, -1.5437e-01,  1.7527e-01,  8.9896e-02,  9.0059e-02,\n",
       "                        1.0628e-01,  8.6865e-02],\n",
       "                      [ 1.5884e-02, -1.8005e-02, -5.5085e-03,  1.2048e-01, -9.7390e-02,\n",
       "                       -1.2948e-01, -1.0593e-01, -9.4660e-02,  8.7168e-02, -8.1213e-02,\n",
       "                        1.5163e-01, -1.2157e-02,  1.2970e-01, -5.6642e-02,  8.0936e-02,\n",
       "                       -8.2938e-02, -6.5791e-02, -1.1423e-01, -1.3086e-01,  5.1344e-03,\n",
       "                        1.7356e-03,  8.4749e-02,  1.2380e-01,  9.0764e-02, -1.2792e-02,\n",
       "                        2.6264e-02,  7.9920e-02,  1.7250e-01, -9.6983e-02, -1.4512e-01,\n",
       "                        1.2316e-01,  8.6279e-02],\n",
       "                      [ 1.2893e-01, -6.0318e-02,  1.7166e-01,  4.9464e-02, -1.8124e-02,\n",
       "                       -1.0252e-02,  8.6597e-02, -1.0540e-01, -2.8805e-02, -1.0192e-01,\n",
       "                       -1.3255e-01,  1.4259e-01, -1.2611e-01, -8.9578e-02,  1.1546e-01,\n",
       "                        1.0756e-01,  9.4157e-02,  8.2534e-02,  9.4443e-02,  9.2424e-02,\n",
       "                        1.6223e-01, -6.9001e-02,  8.2999e-03, -1.3535e-01, -5.2229e-02,\n",
       "                        5.3894e-02,  1.1754e-01,  3.4774e-02,  6.2821e-02,  1.1602e-01,\n",
       "                        9.1917e-02,  4.9557e-02],\n",
       "                      [-9.3814e-02, -4.5030e-02,  3.4645e-02,  1.6731e-01,  3.9171e-03,\n",
       "                        2.4578e-02, -1.3597e-01,  3.8511e-02, -1.3510e-01, -3.6282e-02,\n",
       "                       -1.1512e-01,  1.4150e-01, -1.9099e-02,  1.2060e-01,  4.6816e-02,\n",
       "                       -1.5747e-01,  2.5654e-02,  1.1272e-01, -7.1417e-02, -7.8946e-02,\n",
       "                       -6.1270e-02,  6.5852e-02,  1.5464e-02,  2.8639e-03,  1.2124e-01,\n",
       "                       -2.0613e-02,  8.2344e-02,  8.7355e-02, -3.9056e-05, -5.9123e-02,\n",
       "                        4.3040e-02,  1.6955e-01],\n",
       "                      [-2.5768e-02, -1.4006e-02,  1.4537e-01,  1.4070e-02,  8.9365e-02,\n",
       "                       -4.9378e-02,  7.0785e-02, -1.1339e-01,  9.3219e-02,  6.7318e-02,\n",
       "                       -9.1369e-02, -1.3931e-02,  8.6616e-03, -1.6112e-01, -9.0641e-02,\n",
       "                        6.3817e-02,  5.5138e-02,  1.1175e-01, -5.2794e-02,  1.7505e-02,\n",
       "                        1.2572e-02, -1.1246e-02,  2.8859e-02, -1.6835e-01,  1.3739e-01,\n",
       "                        1.1153e-01,  8.7389e-02,  7.7540e-02, -1.9316e-02, -1.2720e-02,\n",
       "                       -9.7974e-02, -1.3217e-01],\n",
       "                      [ 4.0158e-02,  2.4073e-02, -3.6539e-02,  7.7617e-02,  2.9995e-03,\n",
       "                        6.9667e-02,  7.9705e-02, -7.0869e-02, -1.1751e-01,  1.5128e-01,\n",
       "                        1.5852e-01, -9.3860e-02,  1.2276e-01,  1.6143e-01,  3.6589e-02,\n",
       "                       -7.3647e-02, -1.7646e-01, -1.3641e-01,  1.0605e-01, -8.2328e-02,\n",
       "                        1.3826e-01, -1.5327e-03, -1.2651e-01,  9.2190e-02, -1.1094e-02,\n",
       "                        1.3399e-01, -2.7051e-02, -2.0348e-02,  7.4042e-02,  4.5663e-02,\n",
       "                       -1.1600e-01,  1.4276e-01],\n",
       "                      [-1.0980e-01, -1.4522e-01, -7.9729e-03,  6.2877e-02, -4.7528e-02,\n",
       "                        6.3206e-04,  6.7028e-02, -7.1028e-02, -3.4789e-02,  8.4794e-02,\n",
       "                        3.7032e-02,  1.5673e-01,  7.2210e-02,  1.6475e-01, -9.0669e-02,\n",
       "                        8.0501e-02, -5.7868e-02, -6.2014e-02, -4.7212e-02,  1.4789e-03,\n",
       "                       -1.4170e-01,  1.7640e-01, -1.2717e-03, -2.2545e-02,  9.9107e-02,\n",
       "                       -4.5701e-02, -1.2150e-01,  1.8831e-02,  1.7570e-01, -6.9335e-02,\n",
       "                       -1.1305e-01, -4.6893e-02],\n",
       "                      [-3.1890e-02,  2.2187e-02,  1.1863e-01,  1.1600e-01,  7.9000e-02,\n",
       "                        1.0080e-01, -1.3268e-01,  9.3130e-02,  1.4582e-01,  9.5993e-03,\n",
       "                        1.0734e-01, -1.6110e-01,  1.2192e-01,  1.3181e-01, -1.6348e-01,\n",
       "                        8.9876e-02, -5.9144e-02,  1.5552e-01,  1.1502e-01,  1.7087e-01,\n",
       "                        3.8430e-04, -1.1005e-01,  1.3171e-01, -1.1574e-02,  1.0224e-01,\n",
       "                        8.7264e-02, -6.8894e-03, -9.3109e-02, -9.2734e-02, -4.8361e-03,\n",
       "                       -1.0986e-01, -4.9511e-02],\n",
       "                      [ 5.5780e-02, -2.5155e-02,  1.6580e-01, -1.2861e-01,  1.1233e-01,\n",
       "                        1.3516e-01, -1.2384e-01,  7.8325e-02, -1.1853e-01, -1.0266e-01,\n",
       "                       -1.0311e-01,  1.3639e-01,  1.5487e-01,  1.6414e-01,  1.4568e-01,\n",
       "                        1.1089e-01, -3.9957e-02, -4.5081e-02, -3.3812e-02,  1.1400e-01,\n",
       "                        1.5615e-01, -1.5875e-01,  1.0482e-01,  3.8637e-02, -1.4864e-01,\n",
       "                       -1.8791e-02, -9.7179e-02,  2.8981e-02,  1.5102e-01,  1.2956e-01,\n",
       "                       -1.5922e-01, -1.5986e-01],\n",
       "                      [ 9.9162e-02,  4.7287e-02, -9.0488e-02, -1.2379e-01, -3.0673e-02,\n",
       "                       -8.9937e-02,  1.4534e-01,  5.8311e-02,  5.5950e-02, -7.4506e-02,\n",
       "                       -1.6294e-01,  8.0354e-02, -8.5336e-02,  1.4878e-01, -7.4368e-02,\n",
       "                        4.1359e-03, -1.6353e-01, -7.9685e-03, -5.8134e-02,  1.1960e-01,\n",
       "                        9.1741e-03, -1.0777e-01, -4.4927e-02, -4.5766e-02, -1.7460e-01,\n",
       "                        1.3341e-01,  3.0980e-02,  3.0870e-02, -7.3672e-02, -1.5865e-01,\n",
       "                       -5.8802e-02, -1.2530e-02]])),\n",
       "             ('sa_head.value.weight',\n",
       "              tensor([[ 9.2525e-02, -1.7231e-01,  1.0584e-02,  6.3913e-02,  9.7903e-02,\n",
       "                       -1.3830e-01, -7.9684e-03, -2.6729e-02, -2.2340e-02,  8.9025e-02,\n",
       "                       -6.5383e-02, -1.5479e-01, -6.9102e-02,  6.7692e-02,  2.1663e-02,\n",
       "                       -1.6562e-01, -4.0575e-03, -1.4298e-01,  1.5170e-01, -5.9721e-02,\n",
       "                       -2.9060e-02,  6.4713e-02,  1.5891e-01,  1.4216e-01,  7.8113e-02,\n",
       "                        1.7510e-01,  1.2036e-01, -1.3040e-01,  4.0989e-02, -8.8958e-02,\n",
       "                       -8.6325e-02,  1.4601e-01],\n",
       "                      [-2.2415e-02,  1.0780e-01, -8.7635e-02, -1.4674e-01,  1.1699e-01,\n",
       "                       -1.7143e-01, -1.1287e-02,  8.4282e-02, -1.3872e-01, -1.6805e-01,\n",
       "                        2.0649e-02, -1.2218e-01, -7.8030e-02,  1.6459e-01,  9.2267e-02,\n",
       "                       -1.4272e-01, -4.6763e-02, -1.1402e-02, -1.0258e-02, -9.1907e-02,\n",
       "                       -7.3609e-02, -1.4811e-01, -1.2739e-01, -1.7406e-02, -7.3987e-02,\n",
       "                        7.6997e-02,  6.9293e-02, -1.2040e-01, -4.2303e-02,  3.0457e-02,\n",
       "                        1.3380e-01,  5.2459e-02],\n",
       "                      [-1.3912e-01,  9.5103e-02,  9.2127e-02,  6.7131e-02, -7.4227e-02,\n",
       "                       -9.2965e-02, -1.0014e-01, -9.5560e-02,  7.4658e-02,  1.7518e-01,\n",
       "                        1.3603e-01, -1.1916e-01, -1.0121e-01, -1.4317e-01,  1.3868e-01,\n",
       "                        1.0956e-01, -1.1050e-01,  7.6337e-02,  1.3613e-01,  2.4233e-02,\n",
       "                        9.2512e-02, -1.2600e-01, -1.2777e-01, -8.5851e-02, -1.2507e-01,\n",
       "                       -8.3688e-02,  7.3361e-02,  9.2839e-02, -1.6814e-01,  4.7163e-02,\n",
       "                       -5.8101e-02,  8.1699e-02],\n",
       "                      [ 1.3709e-01, -1.3477e-01, -1.1171e-01,  1.1717e-01,  1.2252e-01,\n",
       "                        1.3904e-01,  4.2433e-02,  1.1777e-01,  1.5298e-01,  4.8707e-02,\n",
       "                        5.6357e-02,  3.9467e-02,  1.7110e-01,  2.2773e-02, -9.8851e-02,\n",
       "                        3.3101e-02, -1.6078e-01,  1.1082e-01,  8.1479e-02, -1.3954e-01,\n",
       "                        4.6692e-02, -7.4752e-02,  1.0282e-01, -1.6046e-01, -1.7400e-01,\n",
       "                        1.6814e-01, -1.1106e-01,  7.8564e-02,  1.3634e-01, -7.8302e-02,\n",
       "                       -2.4510e-02, -2.8020e-02],\n",
       "                      [ 1.7505e-01,  8.2234e-02,  1.5092e-01,  1.2881e-01,  2.2304e-02,\n",
       "                       -8.0968e-02, -1.7631e-01,  1.5022e-01,  1.5992e-01,  1.7892e-02,\n",
       "                       -2.5064e-03, -1.0482e-01,  1.6241e-01,  1.6086e-01, -1.6601e-01,\n",
       "                       -6.3940e-02,  1.1355e-01, -1.5435e-02, -3.4805e-02, -5.7950e-02,\n",
       "                        3.9757e-02, -5.0428e-02,  4.0242e-02,  1.1074e-02, -2.9576e-02,\n",
       "                        4.9846e-02, -4.7089e-02, -1.6022e-01,  4.5082e-02, -1.1841e-02,\n",
       "                       -6.9462e-02, -8.9025e-02],\n",
       "                      [ 8.8558e-02, -7.5568e-02, -6.1895e-02, -3.5400e-02,  1.7761e-02,\n",
       "                       -2.8964e-02, -5.1925e-02, -1.5752e-01, -8.7727e-02,  1.1982e-01,\n",
       "                        1.5620e-01, -1.2068e-01, -1.4020e-01, -1.1449e-01, -3.1546e-02,\n",
       "                        4.4415e-02, -1.4439e-01, -1.3470e-01, -7.6996e-02, -7.0274e-02,\n",
       "                        1.5542e-01, -6.7681e-02, -8.9136e-03, -8.3722e-02, -1.2867e-01,\n",
       "                       -1.4397e-01, -1.1246e-01, -1.5832e-01, -4.5422e-02,  7.0443e-02,\n",
       "                       -3.1379e-02,  2.0286e-03],\n",
       "                      [ 1.2663e-01,  9.4861e-02, -9.0536e-02,  3.3010e-02,  1.0223e-01,\n",
       "                       -8.4329e-02,  5.5425e-02,  1.7200e-01, -1.6659e-01,  7.4578e-02,\n",
       "                       -5.9952e-02,  8.0963e-02,  3.2286e-03,  8.5558e-02, -6.5803e-02,\n",
       "                        1.5139e-01,  1.6841e-01,  1.3286e-01,  1.6755e-01,  7.8533e-02,\n",
       "                       -1.7641e-01,  1.0523e-01, -4.7894e-02,  1.1599e-01,  1.4720e-01,\n",
       "                       -6.6413e-02, -8.2991e-02, -3.7709e-02,  1.2747e-01,  6.3791e-02,\n",
       "                       -1.5323e-01,  8.0551e-02],\n",
       "                      [-1.2249e-01,  9.2566e-02,  1.1342e-02,  1.5341e-01,  1.5729e-02,\n",
       "                        5.6793e-02,  2.4772e-02, -1.0290e-01,  1.6203e-01,  5.7217e-02,\n",
       "                       -3.1198e-02,  3.4496e-02, -1.2787e-01,  6.0391e-02,  1.4503e-01,\n",
       "                       -1.5473e-01, -1.7136e-01, -8.2071e-02, -9.6726e-02, -9.1992e-02,\n",
       "                       -1.2851e-01, -4.9332e-02,  3.2944e-02,  4.0590e-02, -1.3765e-01,\n",
       "                       -1.7063e-01,  1.3364e-01, -6.1615e-02, -9.6203e-03, -1.5204e-01,\n",
       "                       -3.7813e-02,  1.3191e-01],\n",
       "                      [-7.0819e-02, -4.2110e-02, -1.6368e-01,  1.7132e-01,  3.8275e-02,\n",
       "                        8.1723e-02, -9.6474e-02,  1.3950e-01, -1.4825e-02, -1.0900e-01,\n",
       "                        3.4774e-02,  3.3159e-02,  3.5200e-02, -1.0562e-01, -3.8524e-02,\n",
       "                        6.6611e-02,  1.4829e-01,  1.8136e-02,  8.6683e-02, -1.1687e-01,\n",
       "                        2.9258e-02, -1.5687e-01,  9.2630e-03, -2.2424e-02, -5.1262e-02,\n",
       "                       -8.3813e-02, -2.2634e-02,  3.4647e-02,  8.1813e-02, -8.3074e-02,\n",
       "                       -5.4888e-02, -5.6942e-03],\n",
       "                      [ 9.3049e-02,  2.6866e-02,  3.0632e-02, -3.7736e-02, -1.2684e-01,\n",
       "                        7.9865e-02, -9.7274e-02,  9.0722e-02,  7.3917e-02, -8.8638e-02,\n",
       "                       -7.3490e-02,  7.5973e-02,  1.0979e-03, -1.3716e-01,  1.9388e-02,\n",
       "                       -1.2904e-01, -1.8029e-02, -6.1108e-02, -1.3761e-01, -1.7303e-01,\n",
       "                        1.1771e-01, -4.7574e-02, -3.8381e-02, -7.1382e-02,  7.6588e-02,\n",
       "                        1.3258e-01,  1.3685e-02,  1.3097e-02, -1.5580e-01, -1.4655e-01,\n",
       "                        1.2425e-01, -1.0287e-01],\n",
       "                      [-3.0327e-02,  1.4946e-01,  4.1536e-02, -1.1132e-01,  1.3577e-01,\n",
       "                        4.6279e-02, -1.5403e-01,  1.7457e-01, -3.6262e-02,  1.4098e-01,\n",
       "                        1.0394e-01, -1.5360e-01,  1.4283e-02, -8.4262e-02,  2.4215e-03,\n",
       "                        1.2811e-04,  1.2831e-01, -5.8618e-02, -6.8035e-02, -9.3414e-03,\n",
       "                       -1.9792e-02,  4.1139e-02, -1.3528e-02, -1.1093e-01, -9.1491e-02,\n",
       "                       -5.8328e-02,  9.7553e-02, -1.2950e-01,  2.8262e-02, -5.5645e-02,\n",
       "                        1.3561e-01,  3.6519e-02],\n",
       "                      [-3.0924e-02,  1.1609e-01,  1.1746e-03,  1.4694e-01, -1.2461e-01,\n",
       "                        5.3766e-02, -8.5166e-02,  1.1699e-01,  4.8735e-02, -3.8278e-03,\n",
       "                        1.2677e-01, -1.6197e-01,  9.6221e-02,  2.9694e-02,  1.8604e-02,\n",
       "                        5.5004e-02, -1.0158e-02, -1.6552e-01,  1.5197e-01, -4.8118e-02,\n",
       "                        4.9237e-03, -2.1348e-02,  6.9638e-02, -4.7750e-02,  3.0369e-02,\n",
       "                       -4.3994e-03,  6.2445e-02,  3.6533e-03, -5.1442e-02, -3.7233e-02,\n",
       "                        2.8546e-02, -5.5617e-02],\n",
       "                      [-1.3961e-01,  1.5950e-01, -1.5054e-01,  7.8013e-02, -6.3169e-02,\n",
       "                        4.3528e-02, -1.3082e-01, -6.7729e-02, -5.8455e-02,  6.3244e-02,\n",
       "                        7.0566e-02,  2.5138e-02, -5.8696e-02,  1.7405e-01, -1.4333e-02,\n",
       "                       -5.6482e-02,  1.5416e-01,  2.7893e-03, -1.7225e-01, -1.4503e-01,\n",
       "                       -1.1591e-01, -1.4644e-01, -1.6412e-01,  1.3345e-01,  1.4397e-01,\n",
       "                       -9.5178e-02, -6.5823e-02,  8.5813e-02, -5.6791e-02, -6.8624e-02,\n",
       "                        1.0195e-01, -5.8539e-02],\n",
       "                      [ 1.6526e-02, -1.3641e-01, -7.4259e-02,  1.2114e-01,  2.4306e-02,\n",
       "                        9.3188e-02, -4.1790e-02,  1.1395e-01,  7.7745e-03,  1.0531e-01,\n",
       "                        4.5642e-02,  4.4351e-02,  1.7080e-01,  1.1830e-01, -2.9462e-02,\n",
       "                       -7.6891e-02,  1.0746e-01, -1.0487e-01,  1.6955e-01, -7.5221e-02,\n",
       "                       -6.6384e-02, -1.2001e-01, -1.4276e-01, -8.5341e-02,  1.7327e-01,\n",
       "                        1.5037e-01, -2.3747e-02, -5.2796e-02, -5.8571e-02,  2.1269e-02,\n",
       "                       -1.5138e-02,  4.2173e-02],\n",
       "                      [-1.1726e-01,  3.6228e-02,  1.0726e-01,  8.8567e-02, -3.8413e-02,\n",
       "                       -1.3383e-01,  3.5679e-03, -2.8732e-02,  1.7415e-01,  1.1359e-01,\n",
       "                        7.1107e-02, -1.1524e-01,  8.1657e-02, -1.6770e-01,  1.1860e-01,\n",
       "                       -1.3951e-01,  1.3776e-01,  7.4357e-02,  4.0076e-02,  7.8670e-02,\n",
       "                       -1.2401e-01,  8.0024e-02,  1.0363e-01,  5.6952e-02, -1.6789e-01,\n",
       "                       -7.7620e-02, -9.0319e-02,  5.2098e-02, -3.9102e-02,  1.4399e-01,\n",
       "                        1.5646e-01,  1.6042e-01],\n",
       "                      [ 4.4491e-02,  1.2229e-01, -1.6964e-01, -1.3882e-01, -4.9544e-02,\n",
       "                       -4.9671e-02, -1.0522e-01, -4.7604e-02, -1.3848e-01, -3.1111e-02,\n",
       "                       -3.7139e-02, -6.3481e-02,  9.1489e-02, -8.7405e-02,  3.5873e-02,\n",
       "                        4.0529e-02,  1.3716e-03,  1.5787e-01, -1.7312e-01, -1.2523e-01,\n",
       "                       -5.1546e-02, -1.0270e-01,  8.7640e-03,  9.6145e-02, -1.3605e-01,\n",
       "                        4.2531e-02, -1.2104e-01,  4.4805e-02, -1.4112e-01,  4.8696e-02,\n",
       "                       -6.1347e-02, -8.6341e-03]])),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[ 1.3618e-01, -1.7617e-01,  1.7443e-01,  ..., -3.7349e-01,\n",
       "                       -3.7748e-01, -2.1585e-01],\n",
       "                      [-1.1218e-01, -2.1756e-01, -3.6208e-01,  ..., -4.3816e-01,\n",
       "                       -2.1717e-01,  2.4966e-01],\n",
       "                      [-2.1082e-01, -2.6291e-01,  9.1683e-02,  ..., -8.4115e-02,\n",
       "                       -8.7490e-02, -9.3739e-02],\n",
       "                      ...,\n",
       "                      [-4.5749e-06, -9.8483e-02,  7.0840e-02,  ..., -1.0802e-02,\n",
       "                        1.1574e-01, -3.7172e-01],\n",
       "                      [ 8.4637e-03,  2.9458e-01,  6.4005e-02,  ...,  6.6567e-02,\n",
       "                       -3.6081e-01, -1.0889e-01],\n",
       "                      [ 3.6754e-02, -1.9652e-01,  7.8076e-02,  ...,  2.8874e-01,\n",
       "                       -7.3425e-02, -3.7272e-01]])),\n",
       "             ('lm_head.bias',\n",
       "              tensor([ 0.2159,  0.6612, -0.4016, -0.5986, -0.6768, -0.0633, -0.0580, -0.4103,\n",
       "                      -0.4086, -0.7850,  0.4716, -0.4119, -0.5325, -0.1309, -0.4265, -0.3190,\n",
       "                      -0.2018,  0.1234, -0.2410, -0.5585, -0.3250,  0.2187, -0.5256, -0.2597,\n",
       "                      -0.0950, -0.4688, -0.2603, -0.0083, -0.6096, -0.5858, -0.0677, -0.3227,\n",
       "                      -0.1539, -0.2345, -0.3744, -0.4564, -0.5130, -0.4906, -0.6396,  0.2879,\n",
       "                      -0.2027, -0.2614, -0.0165,  0.4931, -0.0262, -0.2266,  0.0863,  0.5271,\n",
       "                      -0.5014, -0.5426,  0.1611, -0.0241,  0.3127,  0.6315, -0.0440, -0.5479,\n",
       "                       0.2980,  0.4299,  0.2337,  0.1597, -0.3445, -0.2503, -0.6070,  0.0041,\n",
       "                      -0.5598]))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(model_state_path, map_location=\"cpu\")\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6ec4b747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 17873\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35149dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "NIO:\n",
      "If but one of his pockets could speak, woul\n",
      "generated:\n",
      "NIO:\n",
      "If but one of his pockets could speak, woulochen!\n",
      "As or ou Anooma f thin sthe crafeen, a dort\n"
     ]
    }
   ],
   "source": [
    "idx = get_batch('val')[0]\n",
    "print(\"prompt:\")\n",
    "print(decode(idx[0].tolist()))\n",
    "print(\"generated:\")\n",
    "print(decode(model.generate(idx, 50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f022b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
